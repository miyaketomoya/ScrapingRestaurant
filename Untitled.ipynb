{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "201169ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import sleep\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "import os\n",
    "import glob\n",
    "import csv\n",
    "import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "94ad74c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "prefectures = [\n",
    "    \"tokyo\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cb34545",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "143074e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_new_dir():\n",
    "    #ディレクトリ作成(最初)\n",
    "    #data- jobs - エリアの数だけフォルダ\n",
    "    #    - results - エリアの数だけフォルダ \n",
    "    \n",
    "    new_dir_path_url = 'data/area/'\n",
    "    new_dir_path_result = 'data/results/'\n",
    "    os.makedirs(new_dir_path_url, exist_ok=True)\n",
    "    os.makedirs(new_dir_path_result, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8317427d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def __get_restaurant_info(store_url):\n",
    "    sleep(1)\n",
    "    url = 'https://tabelog.com/{}'\n",
    "    url = url.format(store_url)\n",
    "\n",
    "    try:\n",
    "        r = requests.get(url)\n",
    "        r.raise_for_status()\n",
    "    except Exception as e:\n",
    "        return None\n",
    "    sleep(1)\n",
    "\n",
    "    soup = BeautifulSoup(r.content, 'lxml')\n",
    "    \n",
    "    \n",
    "    #店名取得\n",
    "    try:\n",
    "        store_name = soup.find(class_= 'rstinfo-table__name-wrap').get_text().strip()\n",
    "    except Exception as e:\n",
    "        store_name = None\n",
    "        \n",
    "    #ジャンル取得\n",
    "    try:\n",
    "        store_genure = soup.find(\"th\",string=\"ジャンル\").find_next_sibling(\"td\").get_text().strip()\n",
    "    except Exception as e:\n",
    "        store_genure = None\n",
    "    \n",
    "    #住所取得\n",
    "    try: \n",
    "        store_adress = soup.find(\"th\",string=\"住所\").find_next_sibling(\"td\").find(\"p\").get_text()\n",
    "    except Exception as e:\n",
    "        store_adress = None\n",
    "      \n",
    "    #電話番号取得\n",
    "    try:\n",
    "        store_phone_num = soup.find(class_ = 'rstdtl-booking-tel-modal__tel-num').get_text().strip()\n",
    "    except Exception as e:\n",
    "        store_phone_num = None\n",
    "        \n",
    "    #情報をまとめる \n",
    "    store_list = [store_name, store_genure, store_adress, store_phone_num]\n",
    "    \n",
    "    print(store_list)\n",
    "    return store_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "265a4961",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 全てのエリアののurlを取得\n",
    "def get_and_save_all_areas():\n",
    "    \n",
    "    for prefecture in prefectures:\n",
    "        sitemap_url = 'https://tabelog.com/sitemap/{}/'.format(prefecture)\n",
    "        try:\n",
    "            r = requests.get(sitemap_url)\n",
    "            r.raise_for_status()\n",
    "        except Exception as e:\n",
    "            return None\n",
    "\n",
    "        soup = BeautifulSoup(r.content,'lxml')\n",
    "        \n",
    "        all_areas = soup.find(class_='prefarea-content').find_all('a')\n",
    "        \n",
    "        area_urls = []\n",
    "        for area in all_areas:\n",
    "            area_url = area.get('href')\n",
    "            print(area_url)\n",
    "            area_urls.append(area_url)\n",
    "        \n",
    "        #print(area_urls)\n",
    "        \n",
    "        filepath = \"data/area/{}area.csv\".format(prefecture)\n",
    "            \n",
    "        with open(filepath, 'w', newline='') as f:\n",
    "            writer = csv.writer(f)            \n",
    "            for url in area_urls:\n",
    "                writer.writerow([url]) \n",
    "            #f.write(json.dumps(job_ids))\n",
    "            sleep(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a04c4165",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_and_save_area_abc():\n",
    "\n",
    "    for prefecture in prefectures:\n",
    "        all_url = []\n",
    "        filepaths = \"data/area/{}area.csv\".format(prefecture)\n",
    "    \n",
    "        area_urls = []\n",
    "        with open(filepaths, 'r') as f:\n",
    "            reader = csv.reader(f)\n",
    "            for row in reader:\n",
    "                url = row[0]  \n",
    "                area_urls.append(url)             \n",
    "        \n",
    "        for url in area_urls:\n",
    "            try:\n",
    "                r = requests.get(url)\n",
    "                r.raise_for_status()\n",
    "            except Exception as e:\n",
    "                return None\n",
    "            \n",
    "            soup = BeautifulSoup(r.content,'lxml')\n",
    "            on_50_url = soup.find_all(class_='sitemap-50on__item-target')\n",
    "            \n",
    "            \n",
    "            \n",
    "            for url in on_50_url:\n",
    "                url_50 = url.get('href')\n",
    "                all_url.append(url_50)    \n",
    "            sleep(1)\n",
    "\n",
    "        filepath = \"data/area/{}area50on.csv\".format(prefecture)\n",
    "            \n",
    "        with open(filepath, 'w', newline='') as f:\n",
    "            writer = csv.writer(f)            \n",
    "            for url in all_url:\n",
    "                writer.writerow([url]) \n",
    "            #f.write(json.dumps(job_ids))\n",
    "            sleep(1)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e02bdb56",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_and_save_store():\n",
    "    \n",
    "    for prefecture in prefectures:\n",
    "        all_url = []\n",
    "        filepaths = \"data/area/{}area50on.csv\".format(prefecture)\n",
    "        \n",
    "        on_50_urls = []\n",
    "        with open(filepaths, 'r') as f:\n",
    "            reader = csv.reader(f)\n",
    "            for row in reader:\n",
    "                on_50_urls.append(row[0])\n",
    "\n",
    "        \n",
    "        for url in on_50_urls:\n",
    "            try:\n",
    "                r = requests.get(url)\n",
    "                r.raise_for_status()\n",
    "            except Exception as e:\n",
    "                return None\n",
    "            print(url)\n",
    "            \n",
    "            soup = BeautifulSoup(r.content,'lxml')\n",
    "\n",
    "            \n",
    "            result_num_element = soup.find(class_=\"result_num\")\n",
    "            if result_num_element is not None:\n",
    "                result_num = int(result_num_element.find(\"strong\").get_text())\n",
    "                print(result_num)\n",
    "            else:\n",
    "                continue\n",
    "            \n",
    "            for i in range(result_num//200+1):\n",
    "                page = str(i+1)\n",
    "                pageurl = url+'?PG={}'.format(page)\n",
    "                print(pageurl)\n",
    "                try:\n",
    "                    r = requests.get(pageurl)\n",
    "                    r.raise_for_status()\n",
    "                except Exception as e:\n",
    "                    return None\n",
    "                \n",
    "                soup = BeautifulSoup(r.content,'lxml')\n",
    "                \n",
    "                urls = soup.find(class_=\"sitemap-50dtl__list\").find_all(\"a\")\n",
    "                \n",
    "                for store in urls:\n",
    "                    store_url = store.get(\"href\")\n",
    "                    print(store_url)\n",
    "                    all_url.append(store_url)\n",
    "                    \n",
    "                sleep(1)\n",
    "        filepath = \"data/area/{}store.csv\".format(prefecture)\n",
    "            \n",
    "        with open(filepath, 'w', newline='') as f:\n",
    "            writer = csv.writer(f)            \n",
    "            for url in all_url:\n",
    "                writer.writerow([url]) \n",
    "            #f.write(json.dumps(job_ids))\n",
    "            sleep(1)     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a11078f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_and_save_restaurant_infos():\n",
    "    for prefecture in prefectures:\n",
    "        store_urls = []\n",
    "        filepaths = \"data/area/{}store.csv\".format(prefecture)\n",
    "        with open(filepaths, 'r') as f:\n",
    "            reader = csv.reader(f)\n",
    "            \n",
    "        for row in reader:\n",
    "            store_urls.append(row[0])\n",
    "            \n",
    "        for sotre_url in store_urls:\n",
    "            store_info = __get_restaurant_info(store_url)\n",
    "            \n",
    "            filepath = \"data/results/{}info_utf-8.csv\".format(prefecture)\n",
    "            with open(filepath, 'a', newline='',encoding='utf_8_sig') as f:\n",
    "                writer = csv.writer(f)            \n",
    "                writer.writerow(store_info) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b965ba28",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    #make_new_dir()\n",
    "    #get_and_save_all_areas()\n",
    "    #get_and_save_area_abc()\n",
    "    #get_and_save_store()\n",
    "    #get_and_save_company_infos()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1f72643f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://tabelog.com/sitemap/tokyo/A1301-A130101/gi/?PG=1\n"
     ]
    }
   ],
   "source": [
    "url = 'https://tabelog.com/sitemap/tokyo/A1301-A130101/gi/'\n",
    "page = str(1)\n",
    "pageurl = url+'?PG={}'.format(page)\n",
    "print(pageurl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5e44d2b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://tabelog.com/sitemap/tokyo/A1318-A131810/bi/\n",
      "6\n",
      "https://tabelog.com/sitemap/tokyo/A1318-A131810/bi/?PG=1\n",
      "/tokyo/A1318/A131810/13236055/\n",
      "/tokyo/A1318/A131810/13275982/\n",
      "/tokyo/A1318/A131810/13117186/\n",
      "/tokyo/A1318/A131810/13204845/\n",
      "/tokyo/A1318/A131810/13239554/\n",
      "/tokyo/A1318/A131810/13089679/\n",
      "https://tabelog.com/sitemap/tokyo/A1318-A131810/bu/\n",
      "3\n",
      "https://tabelog.com/sitemap/tokyo/A1318-A131810/bu/?PG=1\n",
      "/tokyo/A1318/A131810/13273333/\n",
      "/tokyo/A1318/A131810/13257094/\n",
      "/tokyo/A1318/A131810/13021196/\n"
     ]
    }
   ],
   "source": [
    "url = \"https://tabelog.com/sitemap/tokyo/A1318-A131810/bi/\"\n",
    "\n",
    "r = requests.get(url)\n",
    "r.raise_for_status()\n",
    "\n",
    "print(url)\n",
    "            \n",
    "soup = BeautifulSoup(r.content,'lxml')\n",
    "\n",
    "result_num_element = soup.find(class_=\"result_num\")\n",
    "if result_num_element is not None:\n",
    "    result_num = int(result_num_element.find(\"strong\").get_text())\n",
    "    print(result_num)\n",
    "    # ここで result_num を使用する処理を記述します\n",
    "else:\n",
    "    # 見つからなかった場合の処理を記述します\n",
    "    print(0)\n",
    "\n",
    "for i in range(result_num//200+1):\n",
    "    page = str(i+1)\n",
    "    pageurl = url+'?PG={}'.format(page)\n",
    "    print(pageurl)\n",
    "    try:\n",
    "        r = requests.get(pageurl)\n",
    "        r.raise_for_status()\n",
    "    except Exception as e:\n",
    "        print(\"error\")\n",
    "    \n",
    "    soup = BeautifulSoup(r.content,'lxml')\n",
    "    urls = soup.find(class_=\"sitemap-50dtl__list\").find_all(\"a\")\n",
    "    \n",
    "    for store in urls:\n",
    "        store_url = store.get(\"href\")\n",
    "        print(store_url)\n",
    "        #all_url.append(store_url)\n",
    "    sleep(1)\n",
    "\n",
    "    \n",
    "    \n",
    "url = \"https://tabelog.com/sitemap/tokyo/A1318-A131810/bu/\"\n",
    "r = requests.get(url)\n",
    "r.raise_for_status()\n",
    "\n",
    "print(url)\n",
    "            \n",
    "soup = BeautifulSoup(r.content,'lxml')\n",
    "\n",
    "result_num_element = soup.find(class_=\"result_num\")\n",
    "if result_num_element is not None:\n",
    "    result_num = int(result_num_element.find(\"strong\").get_text())\n",
    "    print(result_num)\n",
    "    # ここで result_num を使用する処理を記述します\n",
    "else:\n",
    "    # 見つからなかった場合の処理を記述します\n",
    "    print(0)\n",
    "\n",
    "for i in range(result_num//200+1):\n",
    "    page = str(i+1)\n",
    "    pageurl = url+'?PG={}'.format(page)\n",
    "    print(pageurl)\n",
    "    try:\n",
    "        r = requests.get(pageurl)\n",
    "        r.raise_for_status()\n",
    "    except Exception as e:\n",
    "        print(\"error\")\n",
    "    \n",
    "    soup = BeautifulSoup(r.content,'lxml')\n",
    "    urls = soup.find(class_=\"sitemap-50dtl__list\").find_all(\"a\")\n",
    "    \n",
    "    for store in urls:\n",
    "        store_url = store.get(\"href\")\n",
    "        print(store_url)\n",
    "        #all_url.append(store_url)\n",
    "    sleep(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8e0d6263",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['洋風居酒屋CHARA（チャラ）', '居酒屋', '東京都小笠原村父島字東町 ', '04998-2-3051']\n"
     ]
    }
   ],
   "source": [
    "a = \"/tokyo/A1331/A133102/13211596/\"\n",
    "\n",
    "store_info = __get_restaurant_info(a)\n",
    "\n",
    "filepath1 = \"data/results/{}info_shift-jis.csv\".format(\"tokyo\")\n",
    "filepath2 = \"data/results/{}info_utf-8.csv\".format(\"tokyo\")\n",
    "\n",
    "with open(filepath1, 'a', newline='',encoding='shift-jis') as f:\n",
    "    writer = csv.writer(f)            \n",
    "    writer.writerow(store_info) \n",
    "\n",
    "with open(filepath2, 'a', newline='',encoding='utf_8_sig') as f:\n",
    "    writer = csv.writer(f)            \n",
    "    writer.writerow(store_info) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8d0329c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['洋風居酒屋CHARA（チャラ）', '居酒屋', '東京都小笠原村父島字東町 ', '04998-2-3051']\n",
      "['民宿南国荘 おやつのにわ', 'ジェラート・アイスクリーム、かき氷、ジューススタンド', '東京都小笠原村父島字西町 ', '04998-2-2295']\n",
      "['民宿 たつみ', 'ホテル', '東京都小笠原村父島字奥村 ', '04998-2-2755']\n",
      "['宿ふく', 'その他', '東京都小笠原村父島字東町 ', '04998-2-3410']\n",
      "['ヤンキータウン', 'バー', '東京都小笠原村父島字奥村 ', '04998-2-3042']\n"
     ]
    }
   ],
   "source": [
    "a = [\"/tokyo/A1331/A133102/13211596/\",\n",
    "     \"/tokyo/A1331/A133102/13225118/\",\n",
    "     \"/tokyo/A1331/A133102/13274144/\",\n",
    "     \"/tokyo/A1331/A133102/13272459/\",\n",
    "     \"/tokyo/A1331/A133102/13095357/\"\n",
    "    ]\n",
    "all_store_info = []\n",
    "\n",
    "for b in a:\n",
    "    store_info = __get_restaurant_info(b)\n",
    "    all_store_info.append(store_info)\n",
    "\n",
    "\n",
    "filepath = \"data/results/{}info_utf-8.csv\".format(\"tokyo\")\n",
    "with open(filepath, 'w', newline='',encoding='utf_8_sig') as f:\n",
    "    writer = csv.writer(f)             \n",
    "    for url in all_store_info:\n",
    "        writer.writerow(url) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c70a19c4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
